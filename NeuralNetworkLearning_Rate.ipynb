{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn7I9qego1vgrddB6YCpOy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saikat-too/Neural_Network_From_Scratch/blob/main/NeuralNetworkSGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saikat Singha"
      ],
      "metadata": {
        "id": "G6gKAvXH4JBr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kE9Nn_n2Bf1t"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense Layer with backward\n",
        "\n",
        "class Layer_Dense:\n",
        "\n",
        "  def __init__(self , inputs , neurons):\n",
        "    self.weight = 0.01 * np.random.randn(inputs , neurons)\n",
        "    self.biases = np.zeros((1 , neurons))\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self , inputs):\n",
        "    self.output = np.dot(inputs , self.weight)+self.biases\n",
        "    self.inputs = inputs\n",
        "\n",
        "  # Backward Pass\n",
        "  def backward(self , dvalues):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T , dvalues)\n",
        "    self.dbiases  = np.sum(dvalues , axis=0 , keepdims=True)\n",
        "    # Gradient on Values\n",
        "    self.dinputs  = np.dot(dvalues , self.weight.T)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tgSoCjx44jAW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLu Activation\n",
        "class Activation_ReLu:\n",
        "\n",
        "  # Forward Pass\n",
        "  def forward(self , inputs):\n",
        "    #Remember input values\n",
        "    self.inputs = inputs\n",
        "    self.outputs = np.maximum(0 , inputs)\n",
        "\n",
        "    #Backward Pass\n",
        "  def backward(self , dvalues):\n",
        "    # Since we need to modify the original value\n",
        "    # Let's make a copy of the value first\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Zero Gradient where input values were negative\n",
        "    self.dinputs[self.inputs <=0] = 0\n"
      ],
      "metadata": {
        "id": "ipHEyi7x8BFI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Activation Function\n",
        "\n",
        "class Softmax_Activation:\n",
        "\n",
        "  #Forward Pass\n",
        "\n",
        "  def forward(self , input):\n",
        "\n",
        "    #Get Unnormalized Probabilities\n",
        "\n",
        "    exp_values = np.exp(input - np.max(input , axis=1 , keepdims=True))\n",
        "\n",
        "    #Normalize them for each sample\n",
        "\n",
        "    Probabilities = exp_values/ np.sum(exp_values , axis=1 , keepdims=True)\n",
        "    self.output = Probabilities\n",
        "\n",
        "  # Backward Pass\n",
        "\n",
        "  def backward(self , dvalues):\n",
        "    # Create unutilized arrays\n",
        "    self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "    # Enumerate outputs and gradients\n",
        "    for index , (single_output , single_dvalues) in enumerate(zip(self.output , dvalues)):\n",
        "        # Flatten output array\n",
        "        single_output = single_output.reshape(-1,1)\n",
        "        # Calculate Jacobian matrix of the output\n",
        "        jacobian_matrix = np.diagflat(single_output) - np.dot(single_output , single_output.T)\n",
        "        # Calculate Sample wise gradient and add it to the sample gradients\n",
        "        self.dinputs[index] = np.dot(jacobian_matrix , single_dvalues)\n"
      ],
      "metadata": {
        "id": "2IDqyOLyH0Ec"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common Loss\n",
        "\n",
        "class Loss:\n",
        "\n",
        "  # Calculate the data and regularization losss\n",
        "  def calculate(self , output , y ):\n",
        "\n",
        "      #Calculate sample loss\n",
        "      sample_loss = self.forward(output , y)\n",
        "\n",
        "      #Calculate mean los\n",
        "      data_loss = np.mean(sample_loss)\n",
        "\n",
        "      return data_loss\n"
      ],
      "metadata": {
        "id": "7DXLNcAFO3Bi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cross Entropy Loss\n",
        "\n",
        "class Loss_CategoricalCrossEntropy(Loss):\n",
        "\n",
        "  #Forward Pass\n",
        "  def forward(self, y_pred, y_true):\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values - only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "    # Backward Pass\n",
        "  def backward(self , dvalues , y_true):\n",
        "      # Number of samples\n",
        "      samples = len(dvalues)\n",
        "      # Number of labels in every sample\n",
        "      # We'll use the first sample to count them\n",
        "      labels = len(dvalues[0])\n",
        "\n",
        "      # If labels are sparse , turn them into one-hot vector\n",
        "      if len(y_true.shape) == 1:\n",
        "        y_true = np.eye(labels)[y_true]\n",
        "      # Calculate Gradient\n",
        "      self.dinputs = -y_true / dvalues\n",
        "      # Normalize Gradient\n",
        "      self.dinputs = self.dinputs / samples\n",
        "\n"
      ],
      "metadata": {
        "id": "yrAXZuYs9Iba"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Classifier -> Combined softmax activation and cross entropy loss for faster backward step\n",
        "\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "  # Create activation and loss function objects\n",
        "  def __init__(self):\n",
        "    self.activation = Softmax_Activation()\n",
        "    self.loss = Loss_CategoricalCrossEntropy()\n",
        "\n",
        "    # Forward Pass\n",
        "  def forward(self ,inputs , y_true):\n",
        "    # Output Layer activation function\n",
        "    self.activation.forward(inputs)\n",
        "    # Set the output\n",
        "    self.output = self.activation.output\n",
        "    # Calculate and return loss value\n",
        "    return self.loss.calculate(self.output , y_true)\n",
        "\n",
        "  def backward(self , dvalues , y_true):\n",
        "\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "    # If labels are one hot coded turn them into discrete values\n",
        "    if len(y_true.shape) == 2:\n",
        "      y_true = np.argmax(y_true , axis=1)\n",
        "    # Copy so that we can safely modify\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Calculate Gradient\n",
        "    self.dinputs[range(samples), y_true] -=1\n",
        "    # Normalize Gradient\n",
        "    self.dinputs = self.dinputs / samples"
      ],
      "metadata": {
        "id": "rNK1IlZROiBe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer_SGD:\n",
        "\n",
        "  # Initiialize optimizer -set settings , learning rate 1 is default for the setting\n",
        "  def __init__(self , learning_rate=1. , decay=0.):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1 + self.decay * self.iterations))\n",
        "\n",
        "\n",
        "  # Update Parameters\n",
        "  def update_params(self , layer):\n",
        "    layer.weight += -self.learning_rate * layer.dweights\n",
        "    layer.biases  += -self.learning_rate * layer.dbiases\n",
        "\n",
        "  # Call once after every parameters update\n",
        "  def post_update_params(self):\n",
        "    self.iterations+=1"
      ],
      "metadata": {
        "id": "1PMxSthvmmyr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pocfJrxsRBMJ",
        "outputId": "0cd10aa2-2b2b-4a42-c20c-365a79f70f47"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nnfs in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnfs) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs"
      ],
      "metadata": {
        "id": "xhT1ZnL3RE-Z"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nnfs.datasets import spiral_data"
      ],
      "metadata": {
        "id": "p28cQ3LFRHyz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Layer_Dense(2 ,64)"
      ],
      "metadata": {
        "id": "iDIxkALvRQZa"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation1 =Activation_ReLu()"
      ],
      "metadata": {
        "id": "fl0fS2aLRwJt"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense2 = Layer_Dense(64,3)"
      ],
      "metadata": {
        "id": "gUIDYhStR5zS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "ez5T9X_MR8dK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Optimizer_SGD()"
      ],
      "metadata": {
        "id": "ejws9Skwnzbm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X , y = spiral_data(samples = 100 , classes=3)"
      ],
      "metadata": {
        "id": "n6twP_hmSJpa"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainiing in loop\n",
        "\n",
        "for epoch in range(10001):\n",
        "   # Perform a forward pass of our training data through this layer\n",
        "   dense1.forward(X)\n",
        "   # Perform a forward pass through activation function\n",
        "   activation1.forward(dense1.output)\n",
        "   # Perform a forward pass through second layer\n",
        "   dense2.forward(activation1.outputs)\n",
        "   # Perform a forward pass through activation/loss function\n",
        "   loss = loss_activation.forward(dense2.output , y)\n",
        "\n",
        "   # Calculate accuracy from output of activation 2 and target\n",
        "   predictions = np.argmax(loss_activation.output , axis=1)\n",
        "   if len(y.shape) == 2:\n",
        "       y = np.argmax(y , axis=1)\n",
        "   accuracy = np.mean(predictions==y)\n",
        "\n",
        "   if not epoch % 100:\n",
        "    print (f'epoch : {epoch},' + f'acc : {accuracy:.3f},' + f'loss : {loss:.3f},' + f'lr : {optimizer.current_learning_rate}')\n",
        "\n",
        "   # Backward Pass\n",
        "   loss_activation.backward(loss_activation.output , y)\n",
        "   dense2.backward(loss_activation.dinputs)\n",
        "   activation1.backward(dense2.dinputs)\n",
        "   dense1.backward(activation1.dinputs)\n",
        "\n",
        "   #Update Weight and biases\n",
        "   optimizer.pre_update_params()\n",
        "   optimizer.update_params(dense1)\n",
        "   optimizer.update_params(dense2)\n",
        "   optimizer.post_update_params()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpbs-IUppcTJ",
        "outputId": "f178d5e8-2e6e-49b3-b833-5d8838dd8863"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 0,acc : 0.333,loss : 1.099,lr : 1.0\n",
            "epoch : 100,acc : 0.460,loss : 1.069,lr : 1.0\n",
            "epoch : 200,acc : 0.450,loss : 1.061,lr : 1.0\n",
            "epoch : 300,acc : 0.460,loss : 1.059,lr : 1.0\n",
            "epoch : 400,acc : 0.460,loss : 1.058,lr : 1.0\n",
            "epoch : 500,acc : 0.457,loss : 1.056,lr : 1.0\n",
            "epoch : 600,acc : 0.457,loss : 1.052,lr : 1.0\n",
            "epoch : 700,acc : 0.447,loss : 1.043,lr : 1.0\n",
            "epoch : 800,acc : 0.397,loss : 1.047,lr : 1.0\n",
            "epoch : 900,acc : 0.380,loss : 1.035,lr : 1.0\n",
            "epoch : 1000,acc : 0.440,loss : 1.016,lr : 1.0\n",
            "epoch : 1100,acc : 0.450,loss : 0.999,lr : 1.0\n",
            "epoch : 1200,acc : 0.470,loss : 0.992,lr : 1.0\n",
            "epoch : 1300,acc : 0.457,loss : 1.000,lr : 1.0\n",
            "epoch : 1400,acc : 0.463,loss : 0.996,lr : 1.0\n",
            "epoch : 1500,acc : 0.477,loss : 0.997,lr : 1.0\n",
            "epoch : 1600,acc : 0.513,loss : 0.990,lr : 1.0\n",
            "epoch : 1700,acc : 0.480,loss : 0.992,lr : 1.0\n",
            "epoch : 1800,acc : 0.460,loss : 0.982,lr : 1.0\n",
            "epoch : 1900,acc : 0.463,loss : 0.971,lr : 1.0\n",
            "epoch : 2000,acc : 0.487,loss : 0.967,lr : 1.0\n",
            "epoch : 2100,acc : 0.470,loss : 0.963,lr : 1.0\n",
            "epoch : 2200,acc : 0.510,loss : 0.934,lr : 1.0\n",
            "epoch : 2300,acc : 0.487,loss : 0.920,lr : 1.0\n",
            "epoch : 2400,acc : 0.517,loss : 0.922,lr : 1.0\n",
            "epoch : 2500,acc : 0.567,loss : 0.915,lr : 1.0\n",
            "epoch : 2600,acc : 0.523,loss : 0.946,lr : 1.0\n",
            "epoch : 2700,acc : 0.553,loss : 0.902,lr : 1.0\n",
            "epoch : 2800,acc : 0.577,loss : 0.903,lr : 1.0\n",
            "epoch : 2900,acc : 0.567,loss : 0.910,lr : 1.0\n",
            "epoch : 3000,acc : 0.597,loss : 0.882,lr : 1.0\n",
            "epoch : 3100,acc : 0.567,loss : 0.887,lr : 1.0\n",
            "epoch : 3200,acc : 0.493,loss : 0.920,lr : 1.0\n",
            "epoch : 3300,acc : 0.603,loss : 0.889,lr : 1.0\n",
            "epoch : 3400,acc : 0.593,loss : 0.891,lr : 1.0\n",
            "epoch : 3500,acc : 0.607,loss : 0.873,lr : 1.0\n",
            "epoch : 3600,acc : 0.577,loss : 0.880,lr : 1.0\n",
            "epoch : 3700,acc : 0.610,loss : 0.882,lr : 1.0\n",
            "epoch : 3800,acc : 0.617,loss : 0.875,lr : 1.0\n",
            "epoch : 3900,acc : 0.590,loss : 0.931,lr : 1.0\n",
            "epoch : 4000,acc : 0.553,loss : 0.861,lr : 1.0\n",
            "epoch : 4100,acc : 0.570,loss : 0.890,lr : 1.0\n",
            "epoch : 4200,acc : 0.580,loss : 0.853,lr : 1.0\n",
            "epoch : 4300,acc : 0.553,loss : 0.872,lr : 1.0\n",
            "epoch : 4400,acc : 0.600,loss : 0.842,lr : 1.0\n",
            "epoch : 4500,acc : 0.580,loss : 0.867,lr : 1.0\n",
            "epoch : 4600,acc : 0.623,loss : 0.848,lr : 1.0\n",
            "epoch : 4700,acc : 0.593,loss : 0.823,lr : 1.0\n",
            "epoch : 4800,acc : 0.573,loss : 0.879,lr : 1.0\n",
            "epoch : 4900,acc : 0.627,loss : 0.881,lr : 1.0\n",
            "epoch : 5000,acc : 0.593,loss : 0.814,lr : 1.0\n",
            "epoch : 5100,acc : 0.587,loss : 0.820,lr : 1.0\n",
            "epoch : 5200,acc : 0.590,loss : 0.833,lr : 1.0\n",
            "epoch : 5300,acc : 0.583,loss : 0.862,lr : 1.0\n",
            "epoch : 5400,acc : 0.630,loss : 0.805,lr : 1.0\n",
            "epoch : 5500,acc : 0.610,loss : 0.836,lr : 1.0\n",
            "epoch : 5600,acc : 0.590,loss : 0.797,lr : 1.0\n",
            "epoch : 5700,acc : 0.523,loss : 0.879,lr : 1.0\n",
            "epoch : 5800,acc : 0.610,loss : 0.821,lr : 1.0\n",
            "epoch : 5900,acc : 0.570,loss : 0.877,lr : 1.0\n",
            "epoch : 6000,acc : 0.603,loss : 0.811,lr : 1.0\n",
            "epoch : 6100,acc : 0.607,loss : 0.821,lr : 1.0\n",
            "epoch : 6200,acc : 0.643,loss : 0.782,lr : 1.0\n",
            "epoch : 6300,acc : 0.653,loss : 0.779,lr : 1.0\n",
            "epoch : 6400,acc : 0.583,loss : 0.847,lr : 1.0\n",
            "epoch : 6500,acc : 0.643,loss : 0.799,lr : 1.0\n",
            "epoch : 6600,acc : 0.607,loss : 0.791,lr : 1.0\n",
            "epoch : 6700,acc : 0.653,loss : 0.777,lr : 1.0\n",
            "epoch : 6800,acc : 0.657,loss : 0.777,lr : 1.0\n",
            "epoch : 6900,acc : 0.560,loss : 0.878,lr : 1.0\n",
            "epoch : 7000,acc : 0.597,loss : 0.804,lr : 1.0\n",
            "epoch : 7100,acc : 0.647,loss : 0.782,lr : 1.0\n",
            "epoch : 7200,acc : 0.633,loss : 0.766,lr : 1.0\n",
            "epoch : 7300,acc : 0.573,loss : 0.856,lr : 1.0\n",
            "epoch : 7400,acc : 0.637,loss : 0.791,lr : 1.0\n",
            "epoch : 7500,acc : 0.623,loss : 0.790,lr : 1.0\n",
            "epoch : 7600,acc : 0.657,loss : 0.780,lr : 1.0\n",
            "epoch : 7700,acc : 0.630,loss : 0.794,lr : 1.0\n",
            "epoch : 7800,acc : 0.600,loss : 0.799,lr : 1.0\n",
            "epoch : 7900,acc : 0.657,loss : 0.775,lr : 1.0\n",
            "epoch : 8000,acc : 0.563,loss : 0.872,lr : 1.0\n",
            "epoch : 8100,acc : 0.620,loss : 0.807,lr : 1.0\n",
            "epoch : 8200,acc : 0.650,loss : 0.783,lr : 1.0\n",
            "epoch : 8300,acc : 0.613,loss : 0.790,lr : 1.0\n",
            "epoch : 8400,acc : 0.657,loss : 0.776,lr : 1.0\n",
            "epoch : 8500,acc : 0.567,loss : 0.880,lr : 1.0\n",
            "epoch : 8600,acc : 0.627,loss : 0.799,lr : 1.0\n",
            "epoch : 8700,acc : 0.630,loss : 0.799,lr : 1.0\n",
            "epoch : 8800,acc : 0.613,loss : 0.794,lr : 1.0\n",
            "epoch : 8900,acc : 0.653,loss : 0.772,lr : 1.0\n",
            "epoch : 9000,acc : 0.617,loss : 0.784,lr : 1.0\n",
            "epoch : 9100,acc : 0.580,loss : 0.852,lr : 1.0\n",
            "epoch : 9200,acc : 0.627,loss : 0.818,lr : 1.0\n",
            "epoch : 9300,acc : 0.617,loss : 0.797,lr : 1.0\n",
            "epoch : 9400,acc : 0.607,loss : 0.778,lr : 1.0\n",
            "epoch : 9500,acc : 0.620,loss : 0.757,lr : 1.0\n",
            "epoch : 9600,acc : 0.653,loss : 0.763,lr : 1.0\n",
            "epoch : 9700,acc : 0.657,loss : 0.804,lr : 1.0\n",
            "epoch : 9800,acc : 0.603,loss : 0.819,lr : 1.0\n",
            "epoch : 9900,acc : 0.653,loss : 0.795,lr : 1.0\n",
            "epoch : 10000,acc : 0.607,loss : 0.775,lr : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Rate ->> https://youtu.be/jWT-AX9677k?si=KJoqyZZyExJKqL7U\n",
        "\n",
        "https://www.reddit.com/r/datascience/comments/tgualj/is_there_any_solid_scientific_way_of_choosing/"
      ],
      "metadata": {
        "id": "ol-SaClVLBip"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fnzL1zVSothZ"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}
