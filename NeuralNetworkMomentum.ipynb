{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2onpRj2MawpEvljy1BRcv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saikat-too/Neural_Network_From_Scratch/blob/main/NeuralNetworkMomentum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saikat Singha"
      ],
      "metadata": {
        "id": "G6gKAvXH4JBr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kE9Nn_n2Bf1t"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense Layer with backward\n",
        "\n",
        "class Layer_Dense:\n",
        "\n",
        "  def __init__(self , inputs , neurons):\n",
        "    self.weight = 0.01 * np.random.randn(inputs , neurons)\n",
        "    self.biases = np.zeros((1 , neurons))\n",
        "\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self , inputs):\n",
        "    inputs_copy = np.copy(inputs)\n",
        "    self.output = np.dot(inputs_copy , self.weight)+self.biases\n",
        "    self.inputs = inputs\n",
        "    inputs_copy = np.empty(0)\n",
        "  # Backward Pass\n",
        "  def backward(self , dvalues):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T , dvalues)\n",
        "    self.dbiases  = np.sum(dvalues , axis=0 , keepdims=True)\n",
        "    # Gradient on Values\n",
        "    self.dinputs  = np.dot(dvalues , self.weight.T)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tgSoCjx44jAW"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLu Activation\n",
        "class Activation_ReLu:\n",
        "\n",
        "  # Forward Pass\n",
        "  def forward(self , inputs):\n",
        "    #Remember input values\n",
        "    self.inputs = inputs\n",
        "    self.outputs = np.maximum(0 , inputs)\n",
        "\n",
        "    #Backward Pass\n",
        "  def backward(self , dvalues):\n",
        "    # Since we need to modify the original value\n",
        "    # Let's make a copy of the value first\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Zero Gradient where input values were negative\n",
        "    self.dinputs[self.inputs <=0] = 0\n"
      ],
      "metadata": {
        "id": "ipHEyi7x8BFI"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Activation Function\n",
        "\n",
        "class Softmax_Activation:\n",
        "\n",
        "  #Forward Pass\n",
        "\n",
        "  def forward(self , input):\n",
        "\n",
        "    #Get Unnormalized Probabilities\n",
        "\n",
        "    exp_values = np.exp(input - np.max(input , axis=1 , keepdims=True))\n",
        "\n",
        "    #Normalize them for each sample\n",
        "\n",
        "    Probabilities = exp_values/ np.sum(exp_values , axis=1 , keepdims=True)\n",
        "    self.output = Probabilities\n",
        "\n",
        "  # Backward Pass\n",
        "\n",
        "  def backward(self , dvalues):\n",
        "    # Create unutilized arrays\n",
        "    self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "    # Enumerate outputs and gradients\n",
        "    for index , (single_output , single_dvalues) in enumerate(zip(self.output , dvalues)):\n",
        "        # Flatten output array\n",
        "        single_output = single_output.reshape(-1,1)\n",
        "        # Calculate Jacobian matrix of the output\n",
        "        jacobian_matrix = np.diagflat(single_output) - np.dot(single_output , single_output.T)\n",
        "        # Calculate Sample wise gradient and add it to the sample gradients\n",
        "        self.dinputs[index] = np.dot(jacobian_matrix , single_dvalues)\n"
      ],
      "metadata": {
        "id": "2IDqyOLyH0Ec"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common Loss\n",
        "\n",
        "class Loss:\n",
        "\n",
        "  # Calculate the data and regularization losss\n",
        "  def calculate(self , output , y ):\n",
        "\n",
        "      #Calculate sample loss\n",
        "      sample_loss = self.forward(output , y)\n",
        "\n",
        "      #Calculate mean los\n",
        "      data_loss = np.mean(sample_loss)\n",
        "\n",
        "      return data_loss\n"
      ],
      "metadata": {
        "id": "7DXLNcAFO3Bi"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cross Entropy Loss\n",
        "\n",
        "class Loss_CategoricalCrossEntropy(Loss):\n",
        "\n",
        "  #Forward Pass\n",
        "  def forward(self, y_pred, y_true):\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values - only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "    # Backward Pass\n",
        "  def backward(self , dvalues , y_true):\n",
        "      # Number of samples\n",
        "      samples = len(dvalues)\n",
        "      # Number of labels in every sample\n",
        "      # We'll use the first sample to count them\n",
        "      labels = len(dvalues[0])\n",
        "\n",
        "      # If labels are sparse , turn them into one-hot vector\n",
        "      if len(y_true.shape) == 1:\n",
        "        y_true = np.eye(labels)[y_true]\n",
        "      # Calculate Gradient\n",
        "      self.dinputs = -y_true / dvalues\n",
        "      # Normalize Gradient\n",
        "      self.dinputs = self.dinputs / samples\n",
        "\n"
      ],
      "metadata": {
        "id": "yrAXZuYs9Iba"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Classifier -> Combined softmax activation and cross entropy loss for faster backward step\n",
        "\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "  # Create activation and loss function objects\n",
        "  def __init__(self):\n",
        "    self.activation = Softmax_Activation()\n",
        "    self.loss = Loss_CategoricalCrossEntropy()\n",
        "\n",
        "    # Forward Pass\n",
        "  def forward(self ,inputs , y_true):\n",
        "    # Output Layer activation function\n",
        "    self.activation.forward(inputs)\n",
        "    # Set the output\n",
        "    self.output = self.activation.output\n",
        "    # Calculate and return loss value\n",
        "    return self.loss.calculate(self.output , y_true)\n",
        "\n",
        "  def backward(self , dvalues , y_true):\n",
        "\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "    # If labels are one hot coded turn them into discrete values\n",
        "    if len(y_true.shape) == 2:\n",
        "      y_true = np.argmax(y_true , axis=1)\n",
        "    # Copy so that we can safely modify\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Calculate Gradient\n",
        "    self.dinputs[range(samples), y_true] -=1\n",
        "    # Normalize Gradient\n",
        "    self.dinputs = self.dinputs / samples"
      ],
      "metadata": {
        "id": "rNK1IlZROiBe"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer_SGD:\n",
        "\n",
        "  # Initiialize optimizer -set settings , learning rate 1 is default for the setting\n",
        "  def __init__(self , learning_rate=1. , decay=0. , momentum=0.):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations =0\n",
        "    self.momentum = momentum\n",
        "\n",
        "\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1 + self.decay * self.iterations))\n",
        "\n",
        "\n",
        "  # Update Parameters\n",
        "  def update_params(self , layer):\n",
        "\n",
        "    # If we use momentum\n",
        "    if self.momentum:\n",
        "      global weight_updates , bias_updates\n",
        "      # If layer does not contain momentum  arrays create them filled them with zeroes\n",
        "      if not hasattr(layer , 'weight_momentums'):\n",
        "        layer.weight_momentums = np.zeros_like(layer.weight)\n",
        "        # IF there is no momentum arrays for weight and there is no for biases either\n",
        "        layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "        # Build weights with momentum - take previous update multiplied by retain factor and update with current gradients\n",
        "        weight_updates = (self.momentum * layer.weight_momentums) - (self.current_learning_rate * layer.dweights)\n",
        "        layer.weight_momentums = weight_updates\n",
        "\n",
        "        # Build Bias Updates\n",
        "        bias_updates = (self.momentum * layer.bias_momentums) - (self.current_learning_rate * layer.dbiases)\n",
        "        layer.bias_momentums = bias_updates\n",
        "\n",
        "    # Vanila SGD update\n",
        "    else:\n",
        "      weight_updates = -self.current_learning_rate * self.dweights\n",
        "      bias_updates   = -self.current_learning_rate * self.dbiases\n",
        "\n",
        "\n",
        "\n",
        "    # Update weights and biases using either vanila or momentum updates\n",
        "    layer.weight = layer.weight + weight_updates\n",
        "    layer.biases = layer.weight +  bias_updates\n",
        "\n",
        "  # Call once after every parameter updates\n",
        "  def post_update_params(self):\n",
        "    self.iterations+=1\n"
      ],
      "metadata": {
        "id": "1PMxSthvmmyr"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pocfJrxsRBMJ",
        "outputId": "26843eaa-9cf9-458a-ce19-9645a82afaef"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nnfs in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnfs) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs"
      ],
      "metadata": {
        "id": "xhT1ZnL3RE-Z"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nnfs.datasets import spiral_data"
      ],
      "metadata": {
        "id": "p28cQ3LFRHyz"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X , y = spiral_data(samples = 100 , classes = 3)\n"
      ],
      "metadata": {
        "id": "W3Q9zfuCOHVj"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Layer_Dense(2 ,64)\n"
      ],
      "metadata": {
        "id": "iDIxkALvRQZa"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation1 =Activation_ReLu()"
      ],
      "metadata": {
        "id": "fl0fS2aLRwJt"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense2 = Layer_Dense(64,3)"
      ],
      "metadata": {
        "id": "gUIDYhStR5zS"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "ez5T9X_MR8dK"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Optimizer_SGD(decay=1e-3 , momentum=0.5)"
      ],
      "metadata": {
        "id": "ejws9Skwnzbm"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainiing in loop\n",
        "\n",
        "for epoch in range(10001):\n",
        "\n",
        "   # Perform a forward pass of our training data through this layer\n",
        "   print(f\"X shape before forward : {X.shape}\")\n",
        "   dense1.forward(X)\n",
        "   print (f\"X shape after forward : {X.shape}\")\n",
        "   # Perform a forward pass through activation function\n",
        "   activation1.forward(dense1.output)\n",
        "   # Perform a forward pass through second layer\n",
        "   dense2.forward(activation1.outputs)\n",
        "   # Perform a forward pass through activation/loss function\n",
        "   loss = loss_activation.forward(dense2.output , y)\n",
        "\n",
        "   # Calculate accuracy from output of activation 2 and target\n",
        "   predictions = np.argmax(loss_activation.output , axis=1)\n",
        "   if len(y.shape) == 2:\n",
        "       y = np.argmax(y , axis=1)\n",
        "   accuracy = np.mean(predictions==y)\n",
        "\n",
        "   if not epoch % 100:\n",
        "    print (f'epoch : {epoch},' + f'acc : {accuracy:.3f},' + f'loss : {loss:.3f},' + f'lr : {optimizer.current_learning_rate}')\n",
        "\n",
        "   # Backward Pass\n",
        "   loss_activation.backward(loss_activation.output , y)\n",
        "   dense2.backward(loss_activation.dinputs)\n",
        "   activation1.backward(dense2.dinputs)\n",
        "   dense1.backward(activation1.dinputs)\n",
        "\n",
        "   #Update Weight and biases\n",
        "   optimizer.pre_update_params()\n",
        "   print(f\"Shape of dense1 weights befor optimizing : {dense1.weight.shape}\")\n",
        "   optimizer.update_params(dense1)\n",
        "   print(f\"Shape of dense1 weights after optimizing : {dense1.weight.shape}\")\n",
        "   optimizer.update_params(dense2)\n",
        "   optimizer.post_update_params()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "bpbs-IUppcTJ",
        "outputId": "e35f4009-39dc-4739-be96-d07610e36f54"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape before forward : (300, 2)\n",
            "X shape after forward : (300, 2)\n",
            "epoch : 0,acc : 0.347,loss : 1.099,lr : 1.0\n",
            "Shape of dense1 weights befor optimizing : (2, 64)\n",
            "Shape of dense1 weights after optimizing : (2, 64)\n",
            "X shape before forward : (300, 2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (300,64) (2,64) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-88801a71c6da>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0;31m# Perform a forward pass of our training data through this layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"X shape before forward : {X.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m    \u001b[0mdense1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m    \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf\"X shape after forward : {X.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m    \u001b[0;31m# Perform a forward pass through activation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-30dfa2eafb15>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0minputs_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_copy\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0minputs_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (300,64) (2,64) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fnzL1zVSothZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}