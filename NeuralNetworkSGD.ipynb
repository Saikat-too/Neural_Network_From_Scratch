{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/zXf99+2mCy6h4ySXI3Kz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saikat-too/Neural_Network_From_Scratch/blob/main/NeuralNetworkSGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saikat Singha"
      ],
      "metadata": {
        "id": "G6gKAvXH4JBr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kE9Nn_n2Bf1t"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense Layer with backward\n",
        "\n",
        "class Layer_Dense:\n",
        "\n",
        "  def __init__(self , inputs , neurons):\n",
        "    self.weight = 0.01 * np.random.randn(inputs , neurons)\n",
        "    self.biases = np.zeros((1 , neurons))\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self , inputs):\n",
        "    self.output = np.dot(inputs , self.weight)+self.biases\n",
        "    self.inputs = inputs\n",
        "\n",
        "  # Backward Pass\n",
        "  def backward(self , dvalues):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T , dvalues)\n",
        "    self.dbiases  = np.sum(dvalues , axis=0 , keepdims=True)\n",
        "    # Gradient on Values\n",
        "    self.dinputs  = np.dot(dvalues , self.weight.T)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tgSoCjx44jAW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLu Activation\n",
        "class Activation_ReLu:\n",
        "\n",
        "  # Forward Pass\n",
        "  def forward(self , inputs):\n",
        "    #Remember input values\n",
        "    self.inputs = inputs\n",
        "    self.outputs = np.maximum(0 , inputs)\n",
        "\n",
        "    #Backward Pass\n",
        "  def backward(self , dvalues):\n",
        "    # Since we need to modify the original value\n",
        "    # Let's make a copy of the value first\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Zero Gradient where input values were negative\n",
        "    self.dinputs[self.inputs <=0] = 0\n"
      ],
      "metadata": {
        "id": "ipHEyi7x8BFI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Activation Function\n",
        "\n",
        "class Softmax_Activation:\n",
        "\n",
        "  #Forward Pass\n",
        "\n",
        "  def forward(self , input):\n",
        "\n",
        "    #Get Unnormalized Probabilities\n",
        "\n",
        "    exp_values = np.exp(input - np.max(input , axis=1 , keepdims=True))\n",
        "\n",
        "    #Normalize them for each sample\n",
        "\n",
        "    Probabilities = exp_values/ np.sum(exp_values , axis=1 , keepdims=True)\n",
        "    self.output = Probabilities\n",
        "\n",
        "  # Backward Pass\n",
        "\n",
        "  def backward(self , dvalues):\n",
        "    # Create unutilized arrays\n",
        "    self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "    # Enumerate outputs and gradients\n",
        "    for index , (single_output , single_dvalues) in enumerate(zip(self.output , dvalues)):\n",
        "        # Flatten output array\n",
        "        single_output = single_output.reshape(-1,1)\n",
        "        # Calculate Jacobian matrix of the output\n",
        "        jacobian_matrix = np.diagflat(single_output) - np.dot(single_output , single_output.T)\n",
        "        # Calculate Sample wise gradient and add it to the sample gradients\n",
        "        self.dinputs[index] = np.dot(jacobian_matrix , single_dvalues)\n"
      ],
      "metadata": {
        "id": "2IDqyOLyH0Ec"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common Loss\n",
        "\n",
        "class Loss:\n",
        "\n",
        "  # Calculate the data and regularization losss\n",
        "  def calculate(self , output , y ):\n",
        "\n",
        "      #Calculate sample loss\n",
        "      sample_loss = self.forward(output , y)\n",
        "\n",
        "      #Calculate mean los\n",
        "      data_loss = np.mean(sample_loss)\n",
        "\n",
        "      return data_loss\n"
      ],
      "metadata": {
        "id": "7DXLNcAFO3Bi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cross Entropy Loss\n",
        "\n",
        "class Loss_CategoricalCrossEntropy(Loss):\n",
        "\n",
        "  #Forward Pass\n",
        "  def forward(self, y_pred, y_true):\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values - only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "    # Backward Pass\n",
        "  def backward(self , dvalues , y_true):\n",
        "      # Number of samples\n",
        "      samples = len(dvalues)\n",
        "      # Number of labels in every sample\n",
        "      # We'll use the first sample to count them\n",
        "      labels = len(dvalues[0])\n",
        "\n",
        "      # If labels are sparse , turn them into one-hot vector\n",
        "      if len(y_true.shape) == 1:\n",
        "        y_true = np.eye(labels)[y_true]\n",
        "      # Calculate Gradient\n",
        "      self.dinputs = -y_true / dvalues\n",
        "      # Normalize Gradient\n",
        "      self.dinputs = self.dinputs / samples\n",
        "\n"
      ],
      "metadata": {
        "id": "yrAXZuYs9Iba"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Classifier -> Combined softmax activation and cross entropy loss for faster backward step\n",
        "\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "  # Create activation and loss function objects\n",
        "  def __init__(self):\n",
        "    self.activation = Softmax_Activation()\n",
        "    self.loss = Loss_CategoricalCrossEntropy()\n",
        "\n",
        "    # Forward Pass\n",
        "  def forward(self ,inputs , y_true):\n",
        "    # Output Layer activation function\n",
        "    self.activation.forward(inputs)\n",
        "    # Set the output\n",
        "    self.output = self.activation.output\n",
        "    # Calculate and return loss value\n",
        "    return self.loss.calculate(self.output , y_true)\n",
        "\n",
        "  def backward(self , dvalues , y_true):\n",
        "\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "    # If labels are one hot coded turn them into discrete values\n",
        "    if len(y_true.shape) == 2:\n",
        "      y_true = np.argmax(y_true , axis=1)\n",
        "    # Copy so that we can safely modify\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Calculate Gradient\n",
        "    self.dinputs[range(samples), y_true] -=1\n",
        "    # Normalize Gradient\n",
        "    self.dinputs = self.dinputs / samples"
      ],
      "metadata": {
        "id": "rNK1IlZROiBe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer_SGD:\n",
        "\n",
        "  # Initiialize optimizer -set settings , learning rate 1 is default for the setting\n",
        "  def __init__(self , learning_rate=1.0):\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  # Update Parameters\n",
        "  def update_params(self , layer):\n",
        "    layer.weight += -self.learning_rate * layer.dweights\n",
        "    layer.biases  += -self.learning_rate * layer.dbiases"
      ],
      "metadata": {
        "id": "1PMxSthvmmyr"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pocfJrxsRBMJ",
        "outputId": "f50303c3-6871-42c4-ebb8-2cf036b14a9d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnfs) (1.25.2)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs"
      ],
      "metadata": {
        "id": "xhT1ZnL3RE-Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nnfs.datasets import spiral_data"
      ],
      "metadata": {
        "id": "p28cQ3LFRHyz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Layer_Dense(2 ,64)"
      ],
      "metadata": {
        "id": "iDIxkALvRQZa"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation1 =Activation_ReLu()"
      ],
      "metadata": {
        "id": "fl0fS2aLRwJt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense2 = Layer_Dense(64,3)"
      ],
      "metadata": {
        "id": "gUIDYhStR5zS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "ez5T9X_MR8dK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Optimizer_SGD()"
      ],
      "metadata": {
        "id": "ejws9Skwnzbm"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X , y = spiral_data(samples = 100 , classes=3)"
      ],
      "metadata": {
        "id": "n6twP_hmSJpa"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainiing in loop\n",
        "\n",
        "for epoch in range(10001):\n",
        "   # Perform a forward pass of our training data through this layer\n",
        "   dense1.forward(X)\n",
        "   # Perform a forward pass through activation function\n",
        "   activation1.forward(dense1.output)\n",
        "   # Perform a forward pass through second layer\n",
        "   dense2.forward(activation1.outputs)\n",
        "   # Perform a forward pass through activation/loss function\n",
        "   loss = loss_activation.forward(dense2.output , y)\n",
        "\n",
        "   # Calculate accuracy from output of activation 2 and target\n",
        "   predictions = np.argmax(loss_activation.output , axis=1)\n",
        "   if len(y.shape) == 2:\n",
        "       y = np.argmax(y , axis=1)\n",
        "   accuracy = np.mean(predictions==y)\n",
        "\n",
        "   if not epoch % 100:\n",
        "    print (f'epoch : {epoch},' + f'acc : {accuracy:.3f},' + f'loss : {loss:.3f}')\n",
        "\n",
        "   # Backward Pass\n",
        "   loss_activation.backward(loss_activation.output , y)\n",
        "   dense2.backward(loss_activation.dinputs)\n",
        "   activation1.backward(dense2.dinputs)\n",
        "   dense1.backward(activation1.dinputs)\n",
        "\n",
        "   #Update Weight and biases\n",
        "   optimizer.update_params(dense1)\n",
        "   optimizer.update_params(dense2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpbs-IUppcTJ",
        "outputId": "93a9e068-8f60-457a-dc19-1d1a82dbfba3"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 0,acc : 0.343,loss : 1.099\n",
            "epoch : 100,acc : 0.430,loss : 1.078\n",
            "epoch : 200,acc : 0.430,loss : 1.067\n",
            "epoch : 300,acc : 0.430,loss : 1.066\n",
            "epoch : 400,acc : 0.433,loss : 1.065\n",
            "epoch : 500,acc : 0.457,loss : 1.064\n",
            "epoch : 600,acc : 0.460,loss : 1.062\n",
            "epoch : 700,acc : 0.457,loss : 1.060\n",
            "epoch : 800,acc : 0.460,loss : 1.054\n",
            "epoch : 900,acc : 0.460,loss : 1.042\n",
            "epoch : 1000,acc : 0.470,loss : 1.027\n",
            "epoch : 1100,acc : 0.520,loss : 1.012\n",
            "epoch : 1200,acc : 0.493,loss : 0.994\n",
            "epoch : 1300,acc : 0.453,loss : 0.993\n",
            "epoch : 1400,acc : 0.477,loss : 0.983\n",
            "epoch : 1500,acc : 0.477,loss : 0.978\n",
            "epoch : 1600,acc : 0.470,loss : 0.975\n",
            "epoch : 1700,acc : 0.507,loss : 0.956\n",
            "epoch : 1800,acc : 0.480,loss : 0.962\n",
            "epoch : 1900,acc : 0.487,loss : 0.918\n",
            "epoch : 2000,acc : 0.547,loss : 0.903\n",
            "epoch : 2100,acc : 0.537,loss : 0.883\n",
            "epoch : 2200,acc : 0.587,loss : 0.875\n",
            "epoch : 2300,acc : 0.527,loss : 0.886\n",
            "epoch : 2400,acc : 0.607,loss : 0.850\n",
            "epoch : 2500,acc : 0.647,loss : 0.817\n",
            "epoch : 2600,acc : 0.617,loss : 0.783\n",
            "epoch : 2700,acc : 0.650,loss : 0.762\n",
            "epoch : 2800,acc : 0.613,loss : 0.849\n",
            "epoch : 2900,acc : 0.597,loss : 0.817\n",
            "epoch : 3000,acc : 0.650,loss : 0.721\n",
            "epoch : 3100,acc : 0.720,loss : 0.665\n",
            "epoch : 3200,acc : 0.703,loss : 0.723\n",
            "epoch : 3300,acc : 0.667,loss : 0.708\n",
            "epoch : 3400,acc : 0.637,loss : 0.767\n",
            "epoch : 3500,acc : 0.650,loss : 0.708\n",
            "epoch : 3600,acc : 0.667,loss : 0.684\n",
            "epoch : 3700,acc : 0.737,loss : 0.629\n",
            "epoch : 3800,acc : 0.647,loss : 0.688\n",
            "epoch : 3900,acc : 0.690,loss : 0.652\n",
            "epoch : 4000,acc : 0.690,loss : 0.651\n",
            "epoch : 4100,acc : 0.693,loss : 0.633\n",
            "epoch : 4200,acc : 0.710,loss : 0.628\n",
            "epoch : 4300,acc : 0.703,loss : 0.613\n",
            "epoch : 4400,acc : 0.710,loss : 0.617\n",
            "epoch : 4500,acc : 0.703,loss : 0.609\n",
            "epoch : 4600,acc : 0.703,loss : 0.592\n",
            "epoch : 4700,acc : 0.737,loss : 0.594\n",
            "epoch : 4800,acc : 0.700,loss : 0.614\n",
            "epoch : 4900,acc : 0.733,loss : 0.575\n",
            "epoch : 5000,acc : 0.737,loss : 0.580\n",
            "epoch : 5100,acc : 0.773,loss : 0.514\n",
            "epoch : 5200,acc : 0.737,loss : 0.569\n",
            "epoch : 5300,acc : 0.740,loss : 0.567\n",
            "epoch : 5400,acc : 0.740,loss : 0.558\n",
            "epoch : 5500,acc : 0.743,loss : 0.560\n",
            "epoch : 5600,acc : 0.753,loss : 0.548\n",
            "epoch : 5700,acc : 0.777,loss : 0.508\n",
            "epoch : 5800,acc : 0.753,loss : 0.544\n",
            "epoch : 5900,acc : 0.757,loss : 0.538\n",
            "epoch : 6000,acc : 0.753,loss : 0.543\n",
            "epoch : 6100,acc : 0.757,loss : 0.538\n",
            "epoch : 6200,acc : 0.757,loss : 0.534\n",
            "epoch : 6300,acc : 0.757,loss : 0.537\n",
            "epoch : 6400,acc : 0.760,loss : 0.531\n",
            "epoch : 6500,acc : 0.757,loss : 0.530\n",
            "epoch : 6600,acc : 0.757,loss : 0.530\n",
            "epoch : 6700,acc : 0.767,loss : 0.519\n",
            "epoch : 6800,acc : 0.760,loss : 0.525\n",
            "epoch : 6900,acc : 0.767,loss : 0.528\n",
            "epoch : 7000,acc : 0.763,loss : 0.522\n",
            "epoch : 7100,acc : 0.757,loss : 0.535\n",
            "epoch : 7200,acc : 0.753,loss : 0.539\n",
            "epoch : 7300,acc : 0.767,loss : 0.523\n",
            "epoch : 7400,acc : 0.770,loss : 0.517\n",
            "epoch : 7500,acc : 0.753,loss : 0.520\n",
            "epoch : 7600,acc : 0.763,loss : 0.507\n",
            "epoch : 7700,acc : 0.770,loss : 0.519\n",
            "epoch : 7800,acc : 0.770,loss : 0.514\n",
            "epoch : 7900,acc : 0.767,loss : 0.504\n",
            "epoch : 8000,acc : 0.753,loss : 0.534\n",
            "epoch : 8100,acc : 0.767,loss : 0.499\n",
            "epoch : 8200,acc : 0.767,loss : 0.496\n",
            "epoch : 8300,acc : 0.760,loss : 0.500\n",
            "epoch : 8400,acc : 0.723,loss : 0.541\n",
            "epoch : 8500,acc : 0.767,loss : 0.487\n",
            "epoch : 8600,acc : 0.770,loss : 0.489\n",
            "epoch : 8700,acc : 0.793,loss : 0.496\n",
            "epoch : 8800,acc : 0.797,loss : 0.487\n",
            "epoch : 8900,acc : 0.790,loss : 0.497\n",
            "epoch : 9000,acc : 0.830,loss : 0.439\n",
            "epoch : 9100,acc : 0.800,loss : 0.475\n",
            "epoch : 9200,acc : 0.793,loss : 0.479\n",
            "epoch : 9300,acc : 0.803,loss : 0.473\n",
            "epoch : 9400,acc : 0.733,loss : 0.568\n",
            "epoch : 9500,acc : 0.793,loss : 0.467\n",
            "epoch : 9600,acc : 0.797,loss : 0.465\n",
            "epoch : 9700,acc : 0.797,loss : 0.460\n",
            "epoch : 9800,acc : 0.763,loss : 0.464\n",
            "epoch : 9900,acc : 0.780,loss : 0.444\n",
            "epoch : 10000,acc : 0.777,loss : 0.441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent -> https://youtu.be/IHZwWFHWa-w?si=32k-mu7MSUSj_iUI\n"
      ],
      "metadata": {
        "id": "T6xGCtgmyvRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SGD -> https://youtu.be/UmathvAKj80?si=LsSjXg_bcUjPTvu3"
      ],
      "metadata": {
        "id": "ZIYEEhTg0igs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fnzL1zVSothZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}