{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSOYz8pUCSHBevzJ4Sx7SU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saikat-too/Neural_Network_From_Scratch/blob/main/NeuralNetworkSGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saikat Singha"
      ],
      "metadata": {
        "id": "G6gKAvXH4JBr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kE9Nn_n2Bf1t"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense Layer with backward\n",
        "\n",
        "class Layer_Dense:\n",
        "\n",
        "  def __init__(self , inputs , neurons):\n",
        "    self.weight = 0.01 * np.random.randn(inputs , neurons)\n",
        "    self.biases = np.zeros((1 , neurons))\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self , inputs):\n",
        "    self.output = np.dot(inputs , self.weight)+self.biases\n",
        "    self.inputs = inputs\n",
        "\n",
        "  # Backward Pass\n",
        "  def backward(self , dvalues):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T , dvalues)\n",
        "    self.dbiases  = np.sum(dvalues , axis=0 , keepdims=True)\n",
        "    # Gradient on Values\n",
        "    self.dinputs  = np.dot(dvalues , self.weight.T)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tgSoCjx44jAW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLu Activation\n",
        "class Activation_ReLu:\n",
        "\n",
        "  # Forward Pass\n",
        "  def forward(self , inputs):\n",
        "    #Remember input values\n",
        "    self.inputs = inputs\n",
        "    self.outputs = np.maximum(0 , inputs)\n",
        "\n",
        "    #Backward Pass\n",
        "  def backward(self , dvalues):\n",
        "    # Since we need to modify the original value\n",
        "    # Let's make a copy of the value first\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Zero Gradient where input values were negative\n",
        "    self.dinputs[self.inputs <=0] = 0\n"
      ],
      "metadata": {
        "id": "ipHEyi7x8BFI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Activation Function\n",
        "\n",
        "class Softmax_Activation:\n",
        "\n",
        "  #Forward Pass\n",
        "\n",
        "  def forward(self , input):\n",
        "\n",
        "    #Get Unnormalized Probabilities\n",
        "\n",
        "    exp_values = np.exp(input - np.max(input , axis=1 , keepdims=True))\n",
        "\n",
        "    #Normalize them for each sample\n",
        "\n",
        "    Probabilities = exp_values/ np.sum(exp_values , axis=1 , keepdims=True)\n",
        "    self.output = Probabilities\n",
        "\n",
        "  # Backward Pass\n",
        "\n",
        "  def backward(self , dvalues):\n",
        "    # Create unutilized arrays\n",
        "    self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "    # Enumerate outputs and gradients\n",
        "    for index , (single_output , single_dvalues) in enumerate(zip(self.output , dvalues)):\n",
        "        # Flatten output array\n",
        "        single_output = single_output.reshape(-1,1)\n",
        "        # Calculate Jacobian matrix of the output\n",
        "        jacobian_matrix = np.diagflat(single_output) - np.dot(single_output , single_output.T)\n",
        "        # Calculate Sample wise gradient and add it to the sample gradients\n",
        "        self.dinputs[index] = np.dot(jacobian_matrix , single_dvalues)\n"
      ],
      "metadata": {
        "id": "2IDqyOLyH0Ec"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common Loss\n",
        "\n",
        "class Loss:\n",
        "\n",
        "  # Calculate the data and regularization losss\n",
        "  def calculate(self , output , y ):\n",
        "\n",
        "      #Calculate sample loss\n",
        "      sample_loss = self.forward(output , y)\n",
        "\n",
        "      #Calculate mean los\n",
        "      data_loss = np.mean(sample_loss)\n",
        "\n",
        "      return data_loss\n"
      ],
      "metadata": {
        "id": "7DXLNcAFO3Bi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cross Entropy Loss\n",
        "\n",
        "class Loss_CategoricalCrossEntropy(Loss):\n",
        "\n",
        "  #Forward Pass\n",
        "  def forward(self, y_pred, y_true):\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values - only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "    # Backward Pass\n",
        "  def backward(self , dvalues , y_true):\n",
        "      # Number of samples\n",
        "      samples = len(dvalues)\n",
        "      # Number of labels in every sample\n",
        "      # We'll use the first sample to count them\n",
        "      labels = len(dvalues[0])\n",
        "\n",
        "      # If labels are sparse , turn them into one-hot vector\n",
        "      if len(y_true.shape) == 1:\n",
        "        y_true = np.eye(labels)[y_true]\n",
        "      # Calculate Gradient\n",
        "      self.dinputs = -y_true / dvalues\n",
        "      # Normalize Gradient\n",
        "      self.dinputs = self.dinputs / samples\n",
        "\n"
      ],
      "metadata": {
        "id": "yrAXZuYs9Iba"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Classifier -> Combined softmax activation and cross entropy loss for faster backward step\n",
        "\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "  # Create activation and loss function objects\n",
        "  def __init__(self):\n",
        "    self.activation = Softmax_Activation()\n",
        "    self.loss = Loss_CategoricalCrossEntropy()\n",
        "\n",
        "    # Forward Pass\n",
        "  def forward(self ,inputs , y_true):\n",
        "    # Output Layer activation function\n",
        "    self.activation.forward(inputs)\n",
        "    # Set the output\n",
        "    self.output = self.activation.output\n",
        "    # Calculate and return loss value\n",
        "    return self.loss.calculate(self.output , y_true)\n",
        "\n",
        "  def backward(self , dvalues , y_true):\n",
        "\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "    # If labels are one hot coded turn them into discrete values\n",
        "    if len(y_true.shape) == 2:\n",
        "      y_true = np.argmax(y_true , axis=1)\n",
        "    # Copy so that we can safely modify\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Calculate Gradient\n",
        "    self.dinputs[range(samples), y_true] -=1\n",
        "    # Normalize Gradient\n",
        "    self.dinputs = self.dinputs / samples"
      ],
      "metadata": {
        "id": "rNK1IlZROiBe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer_SGD:\n",
        "\n",
        "  # Initiialize optimizer -set settings , learning rate 1 is default for the setting\n",
        "  def __init__(self , learning_rate=1.):\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "\n",
        "\n",
        "  # Update Parameters\n",
        "  def update_params(self , layer):\n",
        "    layer.weight += -self.learning_rate * layer.dweights\n",
        "    layer.biases  += -self.learning_rate * layer.dbiases\n",
        "\n"
      ],
      "metadata": {
        "id": "1PMxSthvmmyr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pocfJrxsRBMJ",
        "outputId": "87611aa1-3bd5-4dd9-fa13-da9c0ac9f317"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnfs) (1.25.2)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs"
      ],
      "metadata": {
        "id": "xhT1ZnL3RE-Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nnfs.datasets import spiral_data"
      ],
      "metadata": {
        "id": "p28cQ3LFRHyz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Layer_Dense(2 ,64)"
      ],
      "metadata": {
        "id": "iDIxkALvRQZa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation1 =Activation_ReLu()"
      ],
      "metadata": {
        "id": "fl0fS2aLRwJt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense2 = Layer_Dense(64,3)"
      ],
      "metadata": {
        "id": "gUIDYhStR5zS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "ez5T9X_MR8dK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Optimizer_SGD()"
      ],
      "metadata": {
        "id": "ejws9Skwnzbm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X , y = spiral_data(samples = 100 , classes=3)"
      ],
      "metadata": {
        "id": "n6twP_hmSJpa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainiing in loop\n",
        "\n",
        "for epoch in range(10001):\n",
        "   # Perform a forward pass of our training data through this layer\n",
        "   dense1.forward(X)\n",
        "   # Perform a forward pass through activation function\n",
        "   activation1.forward(dense1.output)\n",
        "   # Perform a forward pass through second layer\n",
        "   dense2.forward(activation1.outputs)\n",
        "   # Perform a forward pass through activation/loss function\n",
        "   loss = loss_activation.forward(dense2.output , y)\n",
        "\n",
        "   # Calculate accuracy from output of activation 2 and target\n",
        "   predictions = np.argmax(loss_activation.output , axis=1)\n",
        "   if len(y.shape) == 2:\n",
        "       y = np.argmax(y , axis=1)\n",
        "   accuracy = np.mean(predictions==y)\n",
        "\n",
        "   if not epoch % 100:\n",
        "    print (f'epoch : {epoch},' + f'acc : {accuracy:.3f},' + f'loss : {loss:.3f},')\n",
        "\n",
        "   # Backward Pass\n",
        "   loss_activation.backward(loss_activation.output , y)\n",
        "   dense2.backward(loss_activation.dinputs)\n",
        "   activation1.backward(dense2.dinputs)\n",
        "   dense1.backward(activation1.dinputs)\n",
        "\n",
        "   #Update Weight and biases\n",
        "   optimizer.update_params(dense1)\n",
        "   optimizer.update_params(dense2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpbs-IUppcTJ",
        "outputId": "d9ca2fd5-e4e8-4c76-ea02-3270fa1d6a7b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 0,acc : 0.333,loss : 1.099,\n",
            "epoch : 100,acc : 0.397,loss : 1.085,\n",
            "epoch : 200,acc : 0.423,loss : 1.071,\n",
            "epoch : 300,acc : 0.403,loss : 1.070,\n",
            "epoch : 400,acc : 0.413,loss : 1.069,\n",
            "epoch : 500,acc : 0.420,loss : 1.068,\n",
            "epoch : 600,acc : 0.423,loss : 1.065,\n",
            "epoch : 700,acc : 0.417,loss : 1.058,\n",
            "epoch : 800,acc : 0.427,loss : 1.047,\n",
            "epoch : 900,acc : 0.403,loss : 1.045,\n",
            "epoch : 1000,acc : 0.380,loss : 1.036,\n",
            "epoch : 1100,acc : 0.377,loss : 1.021,\n",
            "epoch : 1200,acc : 0.410,loss : 1.004,\n",
            "epoch : 1300,acc : 0.470,loss : 0.990,\n",
            "epoch : 1400,acc : 0.513,loss : 0.978,\n",
            "epoch : 1500,acc : 0.537,loss : 0.977,\n",
            "epoch : 1600,acc : 0.483,loss : 0.974,\n",
            "epoch : 1700,acc : 0.477,loss : 0.960,\n",
            "epoch : 1800,acc : 0.477,loss : 0.974,\n",
            "epoch : 1900,acc : 0.537,loss : 0.949,\n",
            "epoch : 2000,acc : 0.520,loss : 0.968,\n",
            "epoch : 2100,acc : 0.533,loss : 0.931,\n",
            "epoch : 2200,acc : 0.547,loss : 0.939,\n",
            "epoch : 2300,acc : 0.493,loss : 0.921,\n",
            "epoch : 2400,acc : 0.463,loss : 0.963,\n",
            "epoch : 2500,acc : 0.563,loss : 0.919,\n",
            "epoch : 2600,acc : 0.577,loss : 0.904,\n",
            "epoch : 2700,acc : 0.540,loss : 0.919,\n",
            "epoch : 2800,acc : 0.583,loss : 0.892,\n",
            "epoch : 2900,acc : 0.587,loss : 0.876,\n",
            "epoch : 3000,acc : 0.543,loss : 0.913,\n",
            "epoch : 3100,acc : 0.603,loss : 0.892,\n",
            "epoch : 3200,acc : 0.553,loss : 0.888,\n",
            "epoch : 3300,acc : 0.523,loss : 0.890,\n",
            "epoch : 3400,acc : 0.590,loss : 0.884,\n",
            "epoch : 3500,acc : 0.600,loss : 0.895,\n",
            "epoch : 3600,acc : 0.593,loss : 0.849,\n",
            "epoch : 3700,acc : 0.617,loss : 0.869,\n",
            "epoch : 3800,acc : 0.663,loss : 0.809,\n",
            "epoch : 3900,acc : 0.607,loss : 0.801,\n",
            "epoch : 4000,acc : 0.663,loss : 0.776,\n",
            "epoch : 4100,acc : 0.687,loss : 0.760,\n",
            "epoch : 4200,acc : 0.667,loss : 0.786,\n",
            "epoch : 4300,acc : 0.700,loss : 0.726,\n",
            "epoch : 4400,acc : 0.650,loss : 0.762,\n",
            "epoch : 4500,acc : 0.663,loss : 0.756,\n",
            "epoch : 4600,acc : 0.670,loss : 0.705,\n",
            "epoch : 4700,acc : 0.670,loss : 0.716,\n",
            "epoch : 4800,acc : 0.713,loss : 0.652,\n",
            "epoch : 4900,acc : 0.683,loss : 0.686,\n",
            "epoch : 5000,acc : 0.620,loss : 0.818,\n",
            "epoch : 5100,acc : 0.683,loss : 0.681,\n",
            "epoch : 5200,acc : 0.700,loss : 0.641,\n",
            "epoch : 5300,acc : 0.667,loss : 0.664,\n",
            "epoch : 5400,acc : 0.700,loss : 0.662,\n",
            "epoch : 5500,acc : 0.740,loss : 0.632,\n",
            "epoch : 5600,acc : 0.707,loss : 0.663,\n",
            "epoch : 5700,acc : 0.683,loss : 0.677,\n",
            "epoch : 5800,acc : 0.660,loss : 0.659,\n",
            "epoch : 5900,acc : 0.707,loss : 0.640,\n",
            "epoch : 6000,acc : 0.687,loss : 0.793,\n",
            "epoch : 6100,acc : 0.667,loss : 0.641,\n",
            "epoch : 6200,acc : 0.540,loss : 1.207,\n",
            "epoch : 6300,acc : 0.717,loss : 0.637,\n",
            "epoch : 6400,acc : 0.673,loss : 0.633,\n",
            "epoch : 6500,acc : 0.713,loss : 0.639,\n",
            "epoch : 6600,acc : 0.647,loss : 0.720,\n",
            "epoch : 6700,acc : 0.673,loss : 0.627,\n",
            "epoch : 6800,acc : 0.713,loss : 0.644,\n",
            "epoch : 6900,acc : 0.720,loss : 0.632,\n",
            "epoch : 7000,acc : 0.677,loss : 0.621,\n",
            "epoch : 7100,acc : 0.723,loss : 0.595,\n",
            "epoch : 7200,acc : 0.677,loss : 0.618,\n",
            "epoch : 7300,acc : 0.700,loss : 0.641,\n",
            "epoch : 7400,acc : 0.690,loss : 0.612,\n",
            "epoch : 7500,acc : 0.720,loss : 0.651,\n",
            "epoch : 7600,acc : 0.740,loss : 0.619,\n",
            "epoch : 7700,acc : 0.637,loss : 0.687,\n",
            "epoch : 7800,acc : 0.737,loss : 0.627,\n",
            "epoch : 7900,acc : 0.733,loss : 0.616,\n",
            "epoch : 8000,acc : 0.693,loss : 0.613,\n",
            "epoch : 8100,acc : 0.680,loss : 0.851,\n",
            "epoch : 8200,acc : 0.697,loss : 0.616,\n",
            "epoch : 8300,acc : 0.737,loss : 0.618,\n",
            "epoch : 8400,acc : 0.743,loss : 0.604,\n",
            "epoch : 8500,acc : 0.733,loss : 0.611,\n",
            "epoch : 8600,acc : 0.690,loss : 0.614,\n",
            "epoch : 8700,acc : 0.773,loss : 0.551,\n",
            "epoch : 8800,acc : 0.737,loss : 0.610,\n",
            "epoch : 8900,acc : 0.733,loss : 0.616,\n",
            "epoch : 9000,acc : 0.763,loss : 0.593,\n",
            "epoch : 9100,acc : 0.703,loss : 0.610,\n",
            "epoch : 9200,acc : 0.683,loss : 0.609,\n",
            "epoch : 9300,acc : 0.700,loss : 0.926,\n",
            "epoch : 9400,acc : 0.737,loss : 0.610,\n",
            "epoch : 9500,acc : 0.720,loss : 0.599,\n",
            "epoch : 9600,acc : 0.703,loss : 0.625,\n",
            "epoch : 9700,acc : 0.710,loss : 0.599,\n",
            "epoch : 9800,acc : 0.723,loss : 0.592,\n",
            "epoch : 9900,acc : 0.743,loss : 0.584,\n",
            "epoch : 10000,acc : 0.720,loss : 0.612,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GD -> https://youtu.be/IHZwWFHWa-w?si=CXk7zam1avQYenrW\n",
        "SGD -> https://youtu.be/UmathvAKj80?si=aWrhfF6TDS8VhK37"
      ],
      "metadata": {
        "id": "N4GP0PcDbKre"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fnzL1zVSothZ"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}