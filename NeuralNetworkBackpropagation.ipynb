{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNdCQVkxKqrUNrQBhBIJ7q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saikat-too/Neural_Network_From_Scratch/blob/main/NeuralNetworkBackpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saikat Singha"
      ],
      "metadata": {
        "id": "G6gKAvXH4JBr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "kE9Nn_n2Bf1t"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense Layer with backward\n",
        "\n",
        "class Layer_Dense:\n",
        "\n",
        "  def __init__(self , inputs , neurons):\n",
        "    self.weight = 0.01 * np.random.randn(inputs , neurons)\n",
        "    self.biases = np.zeros((1 , neurons))\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self , inputs):\n",
        "    self.output = np.dot(inputs , self.weight)+self.biases\n",
        "    self.inputs = inputs\n",
        "\n",
        "  # Backward Pass\n",
        "  def backward(self , dvalues):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T , dvalues)\n",
        "    self.dbiases  = np.sum(dvalues , axis=0 , keepdims=True)\n",
        "    # Gradient on Values\n",
        "    self.dinputs  = np.dot(dvalues , self.weight.T)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tgSoCjx44jAW"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLu Activation\n",
        "class Activation_ReLu:\n",
        "\n",
        "  # Forward Pass\n",
        "  def forward(self , inputs):\n",
        "    #Remember input values\n",
        "    self.inputs = inputs\n",
        "    self.outputs = np.maximum(0 , inputs)\n",
        "\n",
        "    #Backward Pass\n",
        "  def backward(self , dvalues):\n",
        "    # Since we need to modify the original value\n",
        "    # Let's make a copy of the value first\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Zero Gradient where input values were negative\n",
        "    self.dinputs[self.inputs <=0] = 0\n"
      ],
      "metadata": {
        "id": "ipHEyi7x8BFI"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Activation Function\n",
        "\n",
        "class Softmax_Activation:\n",
        "\n",
        "  #Forward Pass\n",
        "\n",
        "  def forward(self , input):\n",
        "\n",
        "    #Get Unnormalized Probabilities\n",
        "\n",
        "    exp_values = np.exp(input - np.max(input , axis=1 , keepdims=True))\n",
        "\n",
        "    #Normalize them for each sample\n",
        "\n",
        "    Probabilities = exp_values/ np.sum(exp_values , axis=1 , keepdims=True)\n",
        "    self.output = Probabilities\n",
        "\n",
        "  # Backward Pass\n",
        "\n",
        "  def backward(self , dvalues):\n",
        "    # Create unutilized arrays\n",
        "    self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "    # Enumerate outputs and gradients\n",
        "    for index , (single_output , single_dvalues) in enumerate(zip(self.output , dvalues)):\n",
        "        # Flatten output array\n",
        "        single_output = single_output.reshape(-1,1)\n",
        "        # Calculate Jacobian matrix of the output\n",
        "        jacobian_matrix = np.diagflat(single_output) - np.dot(single_output , single_output.T)\n",
        "        # Calculate Sample wise gradient and add it to the sample gradients\n",
        "        self.dinputs[index] = np.dot(jacobian_matrix , single_dvalues)\n",
        ""
      ],
      "metadata": {
        "id": "2IDqyOLyH0Ec"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common Loss\n",
        "\n",
        "class Loss:\n",
        "\n",
        "  # Calculate the data and regularization losss\n",
        "  def calculate(self , output , y ):\n",
        "\n",
        "      #Calculate sample loss\n",
        "      sample_loss = self.forward(output , y)\n",
        "\n",
        "      #Calculate mean los\n",
        "      data_loss = np.mean(sample_loss)\n",
        "\n",
        "      return data_loss\n"
      ],
      "metadata": {
        "id": "7DXLNcAFO3Bi"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cross Entropy Loss\n",
        "\n",
        "class Loss_CategoricalCrossEntropy(Loss):\n",
        "\n",
        "  #Forward Pass\n",
        "  def forward(self, y_pred, y_true):\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values - only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "    # Backward Pass\n",
        "  def backward(self , dvalues , y_true):\n",
        "      # Number of samples\n",
        "      samples = len(dvalues)\n",
        "      # Number of labels in every sample\n",
        "      # We'll use the first sample to count them\n",
        "      labels = len(dvalues[0])\n",
        "\n",
        "      # If labels are sparse , turn them into one-hot vector\n",
        "      if len(y_true.shape) == 1:\n",
        "        y_true = np.eye(labels)[y_true]\n",
        "      # Calculate Gradient\n",
        "      self.dinputs = -y_true / dvalues\n",
        "      # Normalize Gradient\n",
        "      self.dinputs = self.dinputs / samples\n",
        "\n"
      ],
      "metadata": {
        "id": "yrAXZuYs9Iba"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Classifier -> Combined softmax activation and cross entropy loss for faster backward step\n",
        "\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "  # Create activation and loss function objects\n",
        "  def __init__(self):\n",
        "    self.activation = Softmax_Activation()\n",
        "    self.loss = Loss_CategoricalCrossEntropy()\n",
        "\n",
        "    # Forward Pass\n",
        "  def forward(self ,inputs , y_true):\n",
        "    # Output Layer activation function\n",
        "    self.activation.forward(inputs)\n",
        "    # Set the output\n",
        "    self.output = self.activation.output\n",
        "    # Calculate and return loss value\n",
        "    return self.loss.calculate(self.output , y_true)\n",
        "\n",
        "  def backward(self , dvalues , y_true):\n",
        "\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "    # If labels are one hot coded turn them into discrete values\n",
        "    if len(y_true.shape) == 2:\n",
        "      y_true = np.argmax(y_true , axis=1)\n",
        "    # Copy so that we can safely modify\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Calculate Gradient\n",
        "    self.dinputs[range(samples), y_true] -=1\n",
        "    # Normalize Gradient\n",
        "    self.dinputs = self.dinputs / samples"
      ],
      "metadata": {
        "id": "rNK1IlZROiBe"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pocfJrxsRBMJ",
        "outputId": "8a210eae-6011-417e-ec9c-86e73d3b1f59"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nnfs in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnfs) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs"
      ],
      "metadata": {
        "id": "xhT1ZnL3RE-Z"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nnfs.datasets import spiral_data"
      ],
      "metadata": {
        "id": "p28cQ3LFRHyz"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Layer_Dense(2 ,3)"
      ],
      "metadata": {
        "id": "iDIxkALvRQZa"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation1 =Activation_ReLu()"
      ],
      "metadata": {
        "id": "fl0fS2aLRwJt"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense2 = Layer_Dense(3,3)"
      ],
      "metadata": {
        "id": "gUIDYhStR5zS"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "ez5T9X_MR8dK"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X , y = spiral_data(samples = 100 , classes=3)"
      ],
      "metadata": {
        "id": "n6twP_hmSJpa"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense1.forward(X)"
      ],
      "metadata": {
        "id": "2PCXupPuSPJK"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation1.forward(dense1.output)"
      ],
      "metadata": {
        "id": "E7cIYL0xSSAC"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense2.forward(activation1.outputs)"
      ],
      "metadata": {
        "id": "-UEU5SXwSW3q"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_activation.forward(dense2.output , y)"
      ],
      "metadata": {
        "id": "IbzOXWcgSgH6"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_activation.output[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPCP2dB3SpQa",
        "outputId": "d56538fc-42ce-4431-b26f-e80f9cd6839e"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.3333329  0.33333465 0.33333248]\n",
            " [0.33333355 0.33333308 0.3333334 ]\n",
            " [0.3333335  0.33333316 0.33333337]\n",
            " [0.33333436 0.333332   0.33333364]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "print ('loss' , loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOp577hwTAuW",
        "outputId": "37c98bae-bf3e-44b0-f303-29745f971c42"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 1.0986216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = np.argmax(loss_activation.output , axis=1)\n",
        "\n",
        "if len(y.shape) == 2:\n",
        "  y = np.argmax(y , axis=1)\n",
        "accuracy = np.mean(predictions==y)"
      ],
      "metadata": {
        "id": "jONFY--gTFYq"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('acc' , accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh7Sm3xaTfq7",
        "outputId": "1918b992-6833-4c99-fa33-fbd62f994c1a"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Backward Passs\n",
        "\n",
        "loss_activation.backward(loss_activation.output , y)\n",
        "dense2.backward(loss_activation.dinputs)\n",
        "activation1.backward(dense2.dinputs)\n",
        "dense1.backward(activation1.dinputs)"
      ],
      "metadata": {
        "id": "eRmXXaDLTuvL"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Gradients\n",
        "\n",
        "print(dense1.dweights)\n",
        "print(dense1.dbiases)\n",
        "print(dense2.dweights)\n",
        "print(dense2.dbiases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nESis4EpUNFL",
        "outputId": "1fae34be-50cc-42ad-d1ee-9ff0bd5db61e"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-5.0960352e-05 -1.8986563e-04  3.7796417e-05]\n",
            " [ 3.1350408e-04  1.7625358e-04 -1.1716461e-03]]\n",
            "[[ 0.00063679  0.00028276 -0.00094657]]\n",
            "[[ 1.4502851e-04 -2.9231978e-04  1.4729128e-04]\n",
            " [ 6.7305977e-05  1.6400444e-04 -2.3131041e-04]\n",
            " [ 2.6439058e-04  1.2619779e-04 -3.9058836e-04]]\n",
            "[[-7.4376585e-06  2.7806498e-05 -2.0474428e-05]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BackPropagation -> 3Blue1Brown\n",
        "\n",
        "1.https://youtu.be/Ilg3gGewQ5U?si=f9setGMTBWCSH7M6\n",
        "2.https://youtu.be/tIeHLnjs5U8?si=IN8dfdzQKab6SuOu\n"
      ],
      "metadata": {
        "id": "BVYSdc84WNx7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZaybfT5KVsJz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}